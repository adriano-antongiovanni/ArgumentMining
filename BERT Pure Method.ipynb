{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd6795a",
   "metadata": {},
   "source": [
    "# Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d419bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tabulate import tabulate\n",
    "from tqdm import trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d403e87",
   "metadata": {},
   "source": [
    "# Dataframe load and Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b55b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Nan Values\n",
    "df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e5a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset de indexing of the dataframe\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf53d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing some values to make the data more compreensible for the model\n",
    "df = df.replace('PRO', int(1))\n",
    "df = df.replace('CON', int(0))\n",
    "df = df.replace(-1, int(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26878b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new row joiniing topic and claim\n",
    "df['text'] = df['topicText'] + ' [SEP] [CLS] ' + df['claims.claimCorrectedText']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6978bd9b",
   "metadata": {},
   "source": [
    "# DataFrame Separation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18266d31",
   "metadata": {},
   "source": [
    "In this section, I select the important parts of the pandas dataframe, which are: the topic and claim text (str), the topic and claim sentiment classification (int, [-1,1]), the claim and target relation classification (int, [-1,1]), the stance (str, ['PRO', 'CON']) and the split (str, ['train','test']).\n",
    "\n",
    "The texts will be used in the evaluated models, the classifications will be used to reproduce the formula of the refered article, the stance will be used to evaluate those models and the separation will be used to separate the inputs of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6904e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(input('Wich split: Random[1] or IBM[2]? '))\n",
    "method = int(input('Wich method: Pure BERT[1] or IBM[2]? '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2aa34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 1:\n",
    "    text = df['text'].values\n",
    "    labels = df['claims.stance'].values\n",
    "    etapa = 0\n",
    "elif method == 2:\n",
    "    etapa = int(input('Wich step are you in: topic[1], claim[2] or relation[3]? '))\n",
    "    if etapa == 1:\n",
    "        text = df['topicText'].values\n",
    "        labels = df['topicSentiment'].values\n",
    "    elif etapa == 2:\n",
    "        text = df['claims.claimCorrectedText'].values\n",
    "        labels = df['claims.claimSentiment'].values\n",
    "    elif etapa == 3:\n",
    "        text = df['text'].values\n",
    "        labels = df['claims.targetsRelation'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3af2933",
   "metadata": {},
   "source": [
    "# Functions Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b2e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_id = []\n",
    "attention_masks = []\n",
    "\n",
    "def preprocessing(input_text, tokenizer):\n",
    "  '''\n",
    "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
    "    - input_ids: list of token ids\n",
    "    - token_type_ids: list of token type ids\n",
    "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
    "  '''\n",
    "  return tokenizer.encode_plus(\n",
    "                        input_text,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 32,\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt'\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_tp(preds, labels):\n",
    "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
    "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fp(preds, labels):\n",
    "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
    "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_tn(preds, labels):\n",
    "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
    "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_fn(preds, labels):\n",
    "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
    "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
    "\n",
    "def b_metrics(preds, labels):\n",
    "  '''\n",
    "  Returns the following metrics:\n",
    "    - accuracy    = (TP + TN) / N\n",
    "    - precision   = TP / (TP + FP)\n",
    "    - recall      = TP / (TP + FN)\n",
    "    - specificity = TN / (TN + FP)\n",
    "  '''\n",
    "  preds = np.argmax(preds, axis = 1).flatten()\n",
    "  labels = labels.flatten()\n",
    "  tp = b_tp(preds, labels)\n",
    "  tn = b_tn(preds, labels)\n",
    "  fp = b_fp(preds, labels)\n",
    "  fn = b_fn(preds, labels)\n",
    "  b_accuracy = (tp + tn) / len(labels)\n",
    "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
    "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
    "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
    "  return b_accuracy, b_precision, b_recall, b_specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc59f76",
   "metadata": {},
   "source": [
    "# Preparing Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0b21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    'bert-large-uncased', # bert-base-uncased or bert-large-uncased\n",
    "    do_lower_case = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2f402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in text:\n",
    "  encoding_dict = preprocessing(sample, tokenizer)\n",
    "  token_id.append(encoding_dict['input_ids']) \n",
    "  attention_masks.append(encoding_dict['attention_mask'])\n",
    "\n",
    "token_id = torch.cat(token_id, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1bec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.2\n",
    "batch_size = 16\n",
    "\n",
    "# Indices of the train and validation splits stratified by labels\n",
    "if split == 1  and (etapa == 0 or etapa == 1):\n",
    "    train_idx, val_idx = train_test_split(\n",
    "        np.arange(len(labels)),\n",
    "        test_size = val_ratio,\n",
    "        shuffle = True,\n",
    "        stratify = labels)\n",
    "if split == 2:\n",
    "    train_idx = np.array(df.index[df['split'] == 'train'].tolist())\n",
    "    val_idx = np.array(df.index[df['split'] == 'train'].tolist())\n",
    "\n",
    "# Train and validation sets\n",
    "train_set = TensorDataset(token_id[train_idx], \n",
    "                          attention_masks[train_idx], \n",
    "                          labels[train_idx])\n",
    "\n",
    "val_set = TensorDataset(token_id[val_idx], \n",
    "                        attention_masks[val_idx], \n",
    "                        labels[val_idx])\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "            train_set,\n",
    "            sampler = RandomSampler(train_set),\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            val_set,\n",
    "            sampler = SequentialSampler(val_set),\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14240465",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-large-uncased', # bert-base-uncased or bert-large-uncased\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr = 5e-5, #5e-5, 3e-5, 2e-5\n",
    "                              eps = 1e-08\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaf5fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8e7a8c",
   "metadata": {},
   "source": [
    "# Model Runig and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc3b6f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in trange(epochs, desc = 'Epoch'):\n",
    "    \n",
    "    # ========== Training ==========\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Tracking variables\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        train_output = model(b_input_ids, \n",
    "                             token_type_ids = None, \n",
    "                             attention_mask = b_input_mask, \n",
    "                             labels = b_labels)\n",
    "        # Backward pass\n",
    "        train_output.loss.backward()\n",
    "        optimizer.step()\n",
    "        # Update tracking variables\n",
    "        tr_loss += train_output.loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "\n",
    "    # ========== Validation ==========\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    val_accuracy = []\n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_specificity = []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "          # Forward pass\n",
    "          eval_output = model(b_input_ids, \n",
    "                              token_type_ids = None, \n",
    "                              attention_mask = b_input_mask)\n",
    "        logits = eval_output.logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        # Calculate validation metrics\n",
    "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
    "        val_accuracy.append(b_accuracy)\n",
    "        # Update precision only when (tp + fp) !=0; ignore nan\n",
    "        if b_precision != 'nan': val_precision.append(b_precision)\n",
    "        # Update recall only when (tp + fn) !=0; ignore nan\n",
    "        if b_recall != 'nan': val_recall.append(b_recall)\n",
    "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
    "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
    "\n",
    "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
    "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
    "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
    "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
    "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9467056a",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc3bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecionar qual base usar\n",
    "if etapa  == 3 or etapa == 0:\n",
    "    frases = df['text'][val_idx].values.tolist()\n",
    "elif etapa  == 1:\n",
    "    frases = df['topicText'][val_idx].values.tolist()\n",
    "elif etapa == 2:\n",
    "    frases = df['claims.claimCorrectedText'][val_idx].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6849b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = []\n",
    "for i in frases:\n",
    "    new_sentence = i\n",
    "\n",
    "    # We need Token IDs and Attention Mask for inference on the new sentence\n",
    "    test_ids = []\n",
    "    test_attention_mask = []\n",
    "\n",
    "    # Apply the tokenizer\n",
    "    encoding = preprocessing(new_sentence, tokenizer)\n",
    "\n",
    "    # Extract IDs and Attention Mask\n",
    "    test_ids.append(encoding['input_ids'])\n",
    "    test_attention_mask.append(encoding['attention_mask'])\n",
    "    test_ids = torch.cat(test_ids, dim = 0)\n",
    "    test_attention_mask = torch.cat(test_attention_mask, dim = 0)\n",
    "\n",
    "    # Forward pass, calculate logit predictions\n",
    "    with torch.no_grad():\n",
    "      output = model(test_ids.to(device), token_type_ids = None, attention_mask = test_attention_mask.to(device))\n",
    "\n",
    "    prediction = 1 if np.argmax(output.logits.cpu().numpy()).flatten().item() == 1 else 0\n",
    "    \n",
    "    prediction_list.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c19859",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_list = [[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f87ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if etapa == 3 or etapa == 0:\n",
    "    predictions_list[2] = prediction_list\n",
    "elif etapa == 1:\n",
    "    predictions_list[0] = prediction_list\n",
    "elif etapa == 2:\n",
    "    predictions_list[1] = prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4ce3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if method == 1:\n",
    "    print('Accuracy: ', accuracy_score(df['claims.stance'][val_idx], predictions_list[2]))\n",
    "    print('Precision: ', precision_score(df['claims.stance'][val_idx], predictions_list[2]))\n",
    "    print('Recall: ', recall_score(df['claims.stance'][val_idx], predictions_list[2]))\n",
    "    print('Specificity: ',recall_score(df['claims.stance'][val_idx], predictions_list[2], pos_label = 0))\n",
    "\n",
    "if (method == 2 and etapa == 1):\n",
    "    print('Accuracy: ', accuracy_score(df['claims.stance'][val_idx], predictions_list[0]))\n",
    "    print('Precision: ', precision_score(df['claims.stance'][val_idx], predictions_list[0]))\n",
    "    print('Recall: ', recall_score(df['claims.stance'][val_idx], predictions_list[0]))\n",
    "    print('Specificity: ', recall_score(df['claims.stance'][val_idx], predictions_list[0], pos_label = 0))\n",
    "          \n",
    "if (method == 2 and etapa == 2):\n",
    "    print('Accuracy: ', accuracy_score(df['claims.stance'][val_idx], predictions_list[1]))\n",
    "    print('Precision: ', precision_score(df['claims.stance'][val_idx], predictions_list[1]))\n",
    "    print('Recall: ', recall_score(df['claims.stance'][val_idx], predictions_list[1]))\n",
    "    print('Specificity: ', recall_score(df['claims.stance'][val_idx], predictions_list[1], pos_label = 0))\n",
    "          \n",
    "if (method == 2 and etapa == 3):\n",
    "    print('Accuracy: ', accuracy_score(df['claims.stance'][val_idx], predictions_list[2]))\n",
    "    print('Precision: ', precision_score(df['claims.stance'][val_idx], predictions_list[2]))\n",
    "    print('Recall: ', recall_score(df['claims.stance'][val_idx], predictions_list[2]))\n",
    "    print('Specificity: ', recall_score(df['claims.stance'][val_idx], predictions_list[2], pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815e129b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resul_ibm_method = []\n",
    "for i in range(len(predictions_list[0])):\n",
    "    resul_ibm_method.append(output_prediction[0][i]*output_prediction[1][i]*output_prediction[2][i])\n",
    "print('Accuracy: ', accuracy_score(df['claims.stance'][val_idx], resul_ibm_method))\n",
    "print('Precision: ', precision_score(df['claims.stance'][val_idx], resul_ibm_method))\n",
    "print('Recall: ', recall_score(df['claims.stance'][val_idx], resul_ibm_method))\n",
    "print('Specificity: ',recall_score(df['claims.stance'][val_idx], resul_ibm_method, pos_label = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db95ed",
   "metadata": {},
   "source": [
    "# Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d097e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained()\n",
    "model.save_pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c41b65f",
   "metadata": {},
   "source": [
    "# Benchmark - IBM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cc0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507cc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if split == 1:\n",
    "    print('Benchmark for random spit')\n",
    "elif split == 2:\n",
    "    print('Benchmark for original spit')\n",
    "    \n",
    "#df_ibm_all = df.loc[[val_idx.tolist()],['topicTarget', 'claims.claimCorrectedText', 'claims.stance']]\n",
    "#df_ibm = df_ibm_all[val_idx]\n",
    "\n",
    "df_ibm_topic = df['topicTarget'][val_idx]\n",
    "df_ibm_claim = df['claims.claimCorrectedText'][val_idx]\n",
    "df_ibm_target = df['claims.stance'][val_idx]\n",
    "\n",
    "df_ibm_topic.reset_index(drop = True, inplace = True)\n",
    "df_ibm_claim.reset_index(drop = True, inplace = True)\n",
    "\n",
    "sentence_topic_dicts = []\n",
    "for i in range(len(df_ibm_topic)):\n",
    "    dicti = {'topic': df_ibm_topic[i], 'sentence': df_ibm_claim[i]}\n",
    "    sentence_topic_dicts.append(dicti)\n",
    "    \n",
    "\n",
    "#sentence_topic_dicts = df_data.to_dict('records')\n",
    "#list_target = df_target.values.tolist()\n",
    "#list_target_corrected = []\n",
    "#for i in range(len(list_target)):\n",
    " #   list_target_corrected.append(list_target[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833dbbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from debater_python_api.api.debater_api import DebaterApi\n",
    "\n",
    "debater_api = DebaterApi(api_key)\n",
    "pro_con_client = debater_api.get_pro_con_client()\n",
    "\n",
    "scores = pro_con_client.run(sentence_topic_dicts)\n",
    "\n",
    "resp_ibm = []\n",
    "\n",
    "for j in range(len(sentence_topic_dicts)):\n",
    "    if scores[j] > 0:\n",
    "        resp_ibm.append(1)\n",
    "    elif scores[j] < 0:\n",
    "        resp_ibm.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a104ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: ', accuracy_score(df['claims.stance'][val_idx], resp_ibm))\n",
    "print('Precision: ', precision_score(df['claims.stance'][val_idx], resp_ibm))\n",
    "print('Recall: ', recall_score(df['claims.stance'][val_idx], resp_ibm))\n",
    "print('Specificity: ',recall_score(df['claims.stance'][val_idx], resp_ibm, pos_label = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949ae70f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
